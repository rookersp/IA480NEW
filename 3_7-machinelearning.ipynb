{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.7 - Student Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lab is a continuation of the guided labs in Module 3.\n",
    "\n",
    "In this lab, you will create a hyperparameter tuning job to tune the model that you created previously. You will then compare the metrics of the two models.\n",
    "\n",
    "\n",
    "## Introduction to the business scenario\n",
    "\n",
    "You work for a healthcare provider, and want to improve the detection of abnormalities in orthopedic patients. \n",
    "\n",
    "You are tasked with solving this problem by using machine learning (ML). You have access to a dataset that contains six biomechanical features and a target of *normal* or *abnormal*. You can use this dataset to train an ML model to predict if a patient will have an abnormality.\n",
    "\n",
    "\n",
    "## About this dataset\n",
    "\n",
    "This biomedical dataset was built by Dr. Henrique da Mota during a medical residence period in the Group of Applied Research in Orthopaedics (GARO) of the Centre Médico-Chirurgical de Réadaptation des Massues, Lyon, France. The data has been organized in two different, but related, classification tasks. \n",
    "\n",
    "The first task consists in classifying patients as belonging to one of three categories: \n",
    "\n",
    "- *Normal* (100 patients)\n",
    "- *Disk Hernia* (60 patients)\n",
    "- *Spondylolisthesis* (150 patients)\n",
    "\n",
    "For the second task, the categories *Disk Hernia* and *Spondylolisthesis* were merged into a single category that is labeled as *abnormal*. Thus, the second task consists in classifying patients as belonging to one of two categories: *Normal* (100 patients) or *Abnormal* (210 patients).\n",
    "\n",
    "\n",
    "## Attribute information\n",
    "\n",
    "Each patient is represented in the dataset by six biomechanical attributes that are derived from the shape and orientation of the pelvis and lumbar spine (in this order): \n",
    "\n",
    "- Pelvic incidence\n",
    "- Pelvic tilt\n",
    "- Lumbar lordosis angle\n",
    "- Sacral slope\n",
    "- Pelvic radius\n",
    "- Grade of spondylolisthesis\n",
    "\n",
    "The following convention is used for the class labels: \n",
    "- DH (Disk Hernia)\n",
    "- Spondylolisthesis (SL)\n",
    "- Normal (NO) \n",
    "- Abnormal (AB)\n",
    "\n",
    "\n",
    "For more information about this dataset, see the [Vertebral Column dataset webpage](http://archive.ics.uci.edu/ml/datasets/Vertebral+Column).\n",
    "\n",
    "\n",
    "## Dataset attributions\n",
    "\n",
    "This dataset was obtained from:\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository (http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab setup\n",
    "\n",
    "Because this solution is split across several labs in the module, you run the following cells so that you can load the data and train the model to be deployed.\n",
    "\n",
    "**Note:** The setup can take up to 5 minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data, and training, testing and validating the model\n",
    "\n",
    "By running the following cells, the data will be imported, and the model will be trained, tested and validated and ready for use. \n",
    "\n",
    "**Note:** The following cells represent the key steps in the previous labs.\n",
    "\n",
    "In order to tune the model it must be ready, then you can tweak the mdoel with hyperparameters later in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c123510a2993955l7632265t1w843678844747-labbucket-3ru26r12acij'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "import warnings, requests, zipfile, io\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The following cell takes approximately **10** minutes to complete.  Observe the code and how it processes, this will help you to better understand what is going on in the background.  Keep in mind that this cell completes all the steps you did in previous labs in this module including:\n",
    " - Importing the data\n",
    " - Loading the data into a dataframe\n",
    " - Splitting the data into training, test and validation datasets\n",
    " - Uploading the split datasets to S3\n",
    " - Training, testing and validating the model with the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def plot_roc(test_labels, target_predicted_binary):\n",
    "    TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted_binary).ravel()\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    Sensitivity  = float(TP)/(TP+FN)*100\n",
    "    # Specificity or true negative rate\n",
    "    Specificity  = float(TN)/(TN+FP)*100\n",
    "    # Precision or positive predictive value\n",
    "    Precision = float(TP)/(TP+FP)*100\n",
    "    # Negative predictive value\n",
    "    NPV = float(TN)/(TN+FN)*100\n",
    "    # Fall out or false positive rate\n",
    "    FPR = float(FP)/(FP+TN)*100\n",
    "    # False negative rate\n",
    "    FNR = float(FN)/(TP+FN)*100\n",
    "    # False discovery rate\n",
    "    FDR = float(FP)/(TP+FP)*100\n",
    "    # Overall accuracy\n",
    "    ACC = float(TP+TN)/(TP+FP+FN+TN)*100\n",
    "\n",
    "    print(f\"Sensitivity or TPR: {Sensitivity}%\")    \n",
    "    print(f\"Specificity or TNR: {Specificity}%\") \n",
    "    print(f\"Precision: {Precision}%\")   \n",
    "    print(f\"Negative Predictive Value: {NPV}%\")  \n",
    "    print( f\"False Positive Rate: {FPR}%\") \n",
    "    print(f\"False Negative Rate: {FNR}%\")  \n",
    "    print(f\"False Discovery Rate: {FDR}%\" )\n",
    "    print(f\"Accuracy: {ACC}%\") \n",
    "\n",
    "    test_labels = test.iloc[:,0];\n",
    "    print(\"Validation AUC\", roc_auc_score(test_labels, target_predicted_binary) )\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, target_predicted_binary)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # create the axis of thresholds (scores)\n",
    "    ax2 = plt.gca().twinx()\n",
    "    ax2.plot(fpr, thresholds, markeredgecolor='r',linestyle='dashed', color='r')\n",
    "    ax2.set_ylabel('Threshold',color='r')\n",
    "    ax2.set_ylim([thresholds[-1],thresholds[0]])\n",
    "    ax2.set_xlim([fpr[0],fpr[-1]])\n",
    "\n",
    "    print(plt.figure())\n",
    "\n",
    "def plot_confusion_matrix(test_labels, target_predicted):\n",
    "    matrix = confusion_matrix(test_labels, target_predicted)\n",
    "    df_confusion = pd.DataFrame(matrix)\n",
    "    colormap = sns.color_palette(\"BrBG\", 10)\n",
    "    sns.heatmap(df_confusion, annot=True, fmt='.2f', cbar=None, cmap=colormap)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "f_zip = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00212/vertebral_column_data.zip'\n",
    "r = requests.get(f_zip, stream=True)\n",
    "Vertebral_zip = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "Vertebral_zip.extractall()\n",
    "\n",
    "data = arff.loadarff('column_2C_weka.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "class_mapper = {b'Abnormal':1,b'Normal':0}\n",
    "df['class']=df['class'].replace(class_mapper)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "\n",
    "train, test_and_validate = train_test_split(df, test_size=0.2, random_state=42, stratify=df['class'])\n",
    "test, validate = train_test_split(test_and_validate, test_size=0.5, random_state=42, stratify=test_and_validate['class'])\n",
    "\n",
    "prefix='lab3'\n",
    "\n",
    "train_file='vertebral_train.csv'\n",
    "test_file='vertebral_test.csv'\n",
    "validate_file='vertebral_validate.csv'\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "upload_s3_csv(validate_file, 'validate', validate)\n",
    "\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "\n",
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"auc\",\n",
    "             \"objective\": \"binary:logistic\",\n",
    "             \"silent\" : 1}\n",
    "\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                       sagemaker.get_execution_role(),\n",
    "                                       instance_count=1,\n",
    "                                       instance_type='ml.m5.2xlarge',\n",
    "                                       output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())\n",
    "\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}\n",
    "\n",
    "xgb_model.fit(inputs=data_channels, logs=False)\n",
    "\n",
    "batch_X = test.iloc[:,1:];\n",
    "\n",
    "batch_X_file='batch-in.csv'\n",
    "upload_s3_csv(batch_X_file, 'batch-in', batch_X)\n",
    "\n",
    "batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "xgb_transformer = xgb_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m5.2xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Getting model statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you tune the model, re-familiarize yourself with the current model's metrics.\n",
    "\n",
    "The setup performed a batch prediction, so you must read in the results from Amazon Simple Storage Service (Amazon S3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),names=['class'])\n",
    "\n",
    "def binary_convert(x):\n",
    "    threshold = 0.5\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted_binary = target_predicted['class'].apply(binary_convert)\n",
    "test_labels = test.iloc[:,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix and the receiver operating characteristic (ROC) curve for the original model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, target_predicted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(test_labels, target_predicted_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot gives you a starting point. Make a note of the *Validation area under the curve (AUC)*. You will use it later to check your tuned model to see if it's better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating a hyperparameter tuning job\n",
    "\n",
    "A hyperparameter tuning job can take several hours to complete, depending on the value ranges that you provide. To simplify this task, the parameters used in this step are a subset of the recommended ranges. They were tuned to give good results in this lab, without taking multiple hours to complete.\n",
    "\n",
    "For more information about the parameters to tune for XGBoost, see [Tune an XGBoost Model](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html) in the AWS Documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this next cell can take approximately **45** minutes to complete, go ahead and run the cell. You will examine what's happening, and why these hyperparameter ranges were chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker.get_execution_role(), \n",
    "                                    instance_count= 1, # make sure you have limit set for these instances\n",
    "                                    instance_type='ml.m4.xlarge', \n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(eval_metric='error@.40',\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=42)\n",
    "\n",
    "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 100),\n",
    "                         'min_child_weight': ContinuousParameter(1, 5),\n",
    "                         'subsample': ContinuousParameter(0.5, 1),\n",
    "                         'eta': ContinuousParameter(0.1, 0.3),  \n",
    "                         'num_round': IntegerParameter(1,50)\n",
    "                         }\n",
    "\n",
    "objective_metric_name = 'validation:error'\n",
    "objective_type = 'Minimize'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10, # Set this to 10 or above depending upon budget & available time.\n",
    "                            max_parallel_jobs=1,\n",
    "                            objective_type=objective_type,\n",
    "                            early_stopping_type='Auto')\n",
    "\n",
    "tuner.fit(inputs=data_channels, include_cls_metadata=False)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will create the model that you want to tune.\n",
    "\n",
    "```\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                   role=sagemaker.get_execution_role(), \n",
    "                                   instance_count= 1, # make sure you have limit set for these instances\n",
    "                                   instance_type='ml.m4.xlarge', \n",
    "                                   output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "xgb.set_hyperparameters(eval_metric='[error@.40]',\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=42)\n",
    "\n",
    "```                        \n",
    "\n",
    "Notice that the *eval_metric* of the model was changed to *error@.40*, with a goal of minimizing that value. \n",
    "\n",
    "**error** is the binary classification error rate. It's calculated as *#(wrong cases)/#(all cases)*. For predictions, the evaluation will consider the instances that have a prediction value larger than 0.4 to be positive instances, and the others as negative instances.\n",
    "\n",
    "Next, you must specify the hyperparameters that you want to tune, in addition to the ranges that you must select for each parameter.\n",
    "\n",
    "The hyperparameters that have the largest effect on XGBoost objective metrics are: \n",
    "\n",
    "- alpha\n",
    "- min_child_weight\n",
    "- subsample\n",
    "- eta\n",
    "- num_round \n",
    "\n",
    "The recommended tuning ranges can be found in the AWS Documentation at [Tune an XGBoost Model](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html).\n",
    "\n",
    "For this lab, you will use a *subset* of values. These values were obtained by running the tuning job with the full range, then minimizing the range so that you can use fewer iterations to get better performance. Though this practice isn't strictly realistic, it prevents you from waiting several hours in this lab for the tuning job to complete.\n",
    "\n",
    "```\n",
    "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 100),\n",
    "                         'min_child_weight': ContinuousParameter(1, 5),\n",
    "                         'subsample': ContinuousParameter(0.5, 1),\n",
    "                         'eta': ContinuousParameter(0.1, 0.3),  \n",
    "                         'num_round': IntegerParameter(1,50)\n",
    "                         }\n",
    "```\n",
    "\n",
    "\n",
    "You must specify how you are rating the model. You could use several different objective metrics, a subset of which applies to a binary classifcation problem. Because the evaluation metric is **error**, you set the objective to *error*.\n",
    "\n",
    "```\n",
    "objective_metric_name = 'validation:error'\n",
    "objective_type = 'Minimize'\n",
    "```\n",
    "\n",
    "Finally, you run the tuning job.\n",
    "\n",
    "```\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10, # Set this to 10 or above depending upon budget & available time.\n",
    "                            max_parallel_jobs=1,\n",
    "                            objective_type=objective_type,\n",
    "                            early_stopping_type='Auto')\n",
    "\n",
    "tuner.fit(inputs=data_channels, include_cls_metadata=False)\n",
    "tuner.wait()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-exclamation-triangle\" style=\"color:red\"></i> Wait until the training job is finished. It might take up to **45** minutes. While you are waiting, observe the job status in the console, as described in the following instructions.\n",
    "\n",
    "**To monitor hyperparameter optimization jobs:**  \n",
    "\n",
    "1. In the AWS Management Console, on the **Services** menu, choose **Amazon SageMaker**.  \n",
    "2. Choose **Training > Hyperparameter tuning jobs**.  \n",
    "3. You can check the status of each hyperparameter tuning job, its objective metric value, and its logs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training job is finished, check the job and make sure that it completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Investigating the tuning job results\n",
    "\n",
    "Now that the job is complete, there should be 10 completed jobs. One of the jobs should be marked as the best.\n",
    "\n",
    "You can examine the metrics by getting *HyperparameterTuningJobAnalytics* and loading that data into a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "tuner_analytics = HyperparameterTuningJobAnalytics(tuner.latest_tuning_job.name, sagemaker_session=sagemaker.Session())\n",
    "\n",
    "df_tuning_job_analytics = tuner_analytics.dataframe()\n",
    "\n",
    "# Sort the tuning job analytics by the final metrics value\n",
    "df_tuning_job_analytics.sort_values(\n",
    "    by=['FinalObjectiveValue'],\n",
    "    inplace=True,\n",
    "    ascending=False if tuner.objective_type == \"Maximize\" else True)\n",
    "\n",
    "# Show detailed analytics for the top 20 models\n",
    "df_tuning_job_analytics.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see the hyperparameters that were used for each job, along with the score. You could use those parameters and create a model, or you can get the best model from the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attached_tuner = HyperparameterTuner.attach(tuner.latest_tuning_job.name, sagemaker_session=sagemaker.Session())\n",
    "best_training_job = attached_tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you must attach to the best training job and create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "algo_estimator = Estimator.attach(best_training_job)\n",
    "\n",
    "best_algo_model = algo_estimator.create_model(env={'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT':\"text/csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can use the transform method to perform a batch prediction by using your testing data. Remember that the testing data is data that the model has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "xgb_transformer = best_algo_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predicted target and the test labels of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "best_target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),names=['class'])\n",
    "\n",
    "def binary_convert(x):\n",
    "    threshold = 0.5\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "best_target_predicted_binary = best_target_predicted['class'].apply(binary_convert)\n",
    "test_labels = test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a confusion matrix for your `best_target_predicted` and `test_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, best_target_predicted_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ROC chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(test_labels, best_target_predicted_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How do these results differ from the original? Are these results better or worse? \n",
    "\n",
    "You might not always see an improvement. There are a few reasons for this result:\n",
    "\n",
    "- The model might already be good from the initial pass (what counts as *good* is subjective).\n",
    "- You don't have a large amount of data to train with.\n",
    "- You are using a *subset* of the hyperparameter tuning ranges to save time in this lab.\n",
    "\n",
    "Increasing the hyperparameter ranges (as recommended by the documentation) and running more than 30 jobs will typically improve the model. However, this process will take 2-3 hours to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed this lab, and you can now end the lab by following the lab guide instructions.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
